
# üìù Publications 

> üì© denotes corresponding author, üìå denotes co-first author.

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TVCG 2025</div><img src='images/papers/qffusion.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Qffusion: Controllable Portrait Video Editing via Quadrant-Grid Attention Learning](https://arxiv.org/abs/2501.06438)

Maomao Liüìå, **Lijian Linüìå**, Yunfei Liu, Ye Zhu, Yu Li

[**Project**](https://qffusion.github.io/page/) | [**Video**](https://www.youtube.com/watch?v=ubVrLa6CfJo) 
- We propose a novel dual-frame-guided framework for portrait video editing, which propagates fine-grained local modification from the start and end video frames.
- We propose a recursive inference strategy named Quadrant-grid Propagation (QGP), which can stably generate arbitrary-long videos.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/papers/guava.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/abs/2505.03351v1)

Dongbin Zhang, Yunfei Liuüì©, **Lijian Lin**, Ye Zhu, Yang Li, Minghan Qin, Yu Li, Haoqian Wangüì©

[**Project**](https://eastbeanzhang.github.io/GUAVA/) | [**Video**](https://youtu.be/ylyTO34l5d0) 
- ‚ö°Ô∏è Reconstructs 3D upper-body Gaussian avatars from single image in 0.1s 
- ‚è±Ô∏è Supports real-time expressive animation and novel view synthesis at 50FPS !
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/papers/HR-Avatar.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HRAvatar: High-Quality and Relightable Gaussian Head Avatar](http://arxiv.org/abs/2503.08224)

Dongbin Zhang, Yunfei Liu, **Lijian Lin**, Ye Zhu, Kangjie Chen, Minghan Qin, Yu Li, Haoqian Wang

[**Project**](https://eastbeanzhang.github.io/HRAvatar/) | [**Code**](https://github.com/Pixel-Talk/HRAvatar) 
- We propose HRAvatar, a 3D Gaussian Splatting-based method that reconstructs high-fidelity, relightable 3D head avatars from monocular videos by jointly optimizing tracking, deformation, and appearance modeling. 
- By leveraging learnable blendshapes, physically-based shading, and end-to-end optimization, HRAvatar significantly improves head quality and realism under novel lighting conditions.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/papers/iclr25-teaser.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction](https://iclr.cc/virtual/2025/poster/28296)

Yunfei Liu, Lei Zhu, **Lijian Lin**, Ye Zhu, Ailing Zhang, Yu Li

[**Project**](https://tinyurl.com/TEASER-project) | [**Code**](https://github.com/Pixel-Talk/TEASER) 
- A novel approach that achieves more accurate facial expression reconstruction by predicting a hybrid representation of faces from a single image. 
- A multi-scale facial appearance tokenizer and a token-guided neural renderer to generate high-fidelity facial images. The extracted token is interpretable and highly disentangled, enabling various downstream applications.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/iccv23-moda.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions](https://arxiv.org/abs/2307.10008)

Yunfei Liu, **Lijian Lin**, Fei Yu, Changyin Zhou, Yu Li

[**Project**](projects/iccv23-moda/index.html)
- We propose a unified system for multi-person, diverse, and high-fidelity talking portrait video generation. 
- Extensive evaluations demonstrate that the proposed system produces more natural and realistic video portraits compared to previous methods.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/papers/aaai23_mgsr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Accelerating the training of video super-resolution models](https://ojs.aaai.org/index.php/AAAI/article/download/25246/25018)

**Lijian Lin**, Xintao Wang, Zhongang Qi, Ying Shan

- Our method is capable of largely speeding up training (up to speedup in wall-clock training time) without performance drop for various VSR models.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2020</div><img src='images/papers/MM20_DSFNet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[Dual semantic fusion network for video object detection](https://arxiv.org/pdf/2009.07498)

**Lijian Linüìå**, Haosheng Chenüìå, Honglun Zhang, Jun Liang, Yu Li, Ying Shan, Hanzi Wang

- We present a dual semantic fusion network, which performs a multi-granularity semantic fusion at both frame level and instance level in a unified framework and then generates
enhanced features for video object detection.
- We introduce a geometric similarity measure into the proposed dual semantic fusion network along with the widely used appearance similarity measure to alleviate the information distortion caused by noise during the fusion process.
</div>
</div>

- `ICCV 2025` [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/html/2507.02691v1), Xiangyang Luo, ‚ÄÉYe Zhu, ‚ÄÉYunfei Liu,‚ÄÉ**Lijian Lin**, ‚ÄÉCong Wan, ‚ÄÉZijian Cai, Shao-Lun Huang, Yu Li
- `AAAI 2025` [AnyTalk: Multi-modal Driven Multi-domain Talking Head Generation](https://anytalker.github.io/), Yu Wang, Yunfei Liu, Fa-Ting Hong, Meng Cao, **Lijian Lin**, Yu Li
- `ICLR 2024` GPAvatar: Generalizable and Precise Head Avatar from Image(s), Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, **Lijian Lin**, Yunfei Liu, Tatsuya Harada
- `ICCV 2023` [Accurate 3D Face Reconstruction with Facial Component Tokens](https://liuyunfei.net/), Tianke Zhang, Xuangeng Chu, Yunfei Liu, **Lijian Lin**, Zhendong Yang, _et al._.
- `AAAI 2023` [Tagging before alignment: Integrating multi-modal tags for video-text retrieval](https://ojs.aaai.org/index.php/AAAI/article/download/25113/24885), Yizhen Chen, Jie Wang, **Lijian Lin**, Zhongang Qi, Jin Ma, Ying Shan


