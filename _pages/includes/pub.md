
# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/iclr25-teaser.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction](https://iclr.cc/virtual/2025/poster/28296)

**Yunfei Liu**, Lei Zhu, Lijian Lin, Ye Zhu, Ailing Zhang, Yu Li

[**Project**](projects/iclr25/index.html)
- A novel approach that achieves more accurate facial expression reconstruction by predicting a hybrid representation of faces from a single image. 
- A multi-scale facial appearance tokenizer and a token-guided neural renderer to generate high-fidelity facial images. The extracted token is interpretable and highly disentangled, enabling various downstream applications.
</div>
</div>

[DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation](https://arxiv.org/abs/2401.04747)

Junming Chen, **Yunfei Liu**, Jianan Wang, Ailing Zeng, Yu Li, Qifeng Chen

[**Project**](https://jeremycjm.github.io/proj/DiffSHEG)
- We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length.
- Our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/iccv23-moda.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions](https://arxiv.org/abs/2307.10008)

**Yunfei Liu**, Lijian Lin, Fei Yu, Changyin Zhou, Yu Li

[**Project**](projects/iccv23-moda/index.html)
- We propose a unified system for multi-person, diverse, and high-fidelity talking portrait video generation. 
- Extensive evaluations demonstrate that the proposed system produces more natural and realistic video portraits compared to previous methods.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/tpami-23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[First- And Third-person Video Co-analysis By Learning Spatial-temporal Joint Attention](https://ieeexplore.ieee.org/document/9220850)

Huangyue Yu, Minjie Cai, **Yunfei Liu**, Feng Lu

**Project | IF=17.730** 
- We propose a multi-branch deep network, which extracts cross-view joint attention and shared representation from static frames with spatial constraints, in a self-supervised and simultaneous manner.
- We demonstrate how the learnt joint information can benefit various applications.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/papers/cvpr22-gazeonce.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GazeOnce: Real-Time Multi-Person Gaze Estimation](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_GazeOnce_Real-Time_Multi-Person_Gaze_Estimation_CVPR_2022_paper.pdf)

Mingfang Zhang, **Yunfei Liu**, Feng Lu

[**Project**](https://github.com/mf-zhang/GazeOnce) 
- GazeOnce is the first one-stage endto-end gaze estimation method.
- This unified framework not only offers a faster speed, but also provides a lower gaze estimation error compared with other SOTA methods.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2021</div><img src='images/papers/iccv-21.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation](http://arxiv.org/abs/2107.13780)

**Yunfei Liu**\*, Ruicong Liu\*, Haofei Wang, Feng Lu

[**Project**](projects/iccv21/index.html) <strong><span class='show_paper_citations' data='B1Z1vTMAAAAJ:Tyk-4Ss8FVUC'></span></strong>
- PnP-GA is an ensemble of networks that learn collaboratively with the guidance of outliers.
- Existing gaze estimation networks can be directly plugged into PnP-GA and generalize the algorithms to new domains.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2020</div><img src='images/papers/cvpr-20.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[Unsupervised Learning for Intrinsic Image Decomposition from a Single Image](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unsupervised_Learning_for_Intrinsic_Image_Decomposition_From_a_Single_Image_CVPR_2020_paper.pdf)

**Yunfei Liu**, Yu Li, Shaodi You, Feng Lu

[**Project** ![](https://img.shields.io/github/stars/DreamtaleCore/USI3D?style=social)](projects/cvpr20/index.html) <strong><span class='show_paper_citations' data='B1Z1vTMAAAAJ:qjMakFHDy7sC'></span></strong>
- USI3D is the first intrinsic image decomposition method that learns from uncorrelected image sets.
- **Academic Impact:** This work is included by many low-level vision projects, such as [Relighting4D ![](https://img.shields.io/github/stars/FrozenBurning/Relighting4D?style=social)](https://github.com/FrozenBurning/Relighting4D), [IntrinsicHarmony](https://github.com/zhenglab/IntrinsicHarmony), [DIB-R++](https://nv-tlabs.github.io/DIBRPlus/). Discussions in [Zhihu](https://zhuanlan.zhihu.com/p/269632886).
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2020</div><img src='images/papers/eccv-20.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks](https://arxiv.org/abs/2007.02343)

**Yunfei Liu**, Xingju Ma, James Bailey, Feng Lu

[**Project | (Citations 300+**)] <strong><span class='show_paper_citations' data='ueU8eIwAAAAJ:d1gkVwhDpl0C'></span></strong>
- We¬†present¬†a¬†new¬†type¬†of¬†backdoor¬†attack: natural¬†reflection¬†phenomenon.
- **Academic Impact:** This work is included by many backdoor attack/defense works, Such as [NAD ![](https://img.shields.io/github/stars/bboylyg/NAD?style=social)](https://github.com/bboylyg/NAD). This work is at the first place at [google scholar](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=backdoor+attack&btnG=) .
</div>
</div>
- `AAAI 2025` [AnyTalk: Multi-modal Driven Multi-domain Talking Head Generation](https://anytalker.github.io/), Yu Wang, **Yunfei Liu**, Fa-Ting Hong, Meng Cao, Lijian Lin, Yu Li
- `ECCV 2024` [AddMe: Zero-shot Group-photo Synthesis by Inserting People into Scenes](liuyunfei.net), Dongxu Yue, Maomao Li, **Yunfei Liu**, Ailing Zeng, Tianyu Yang, Qin Guo, Yu Li 
- `CVPR 2024` [A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing](https://stem-inv.github.io/page/), Maomao Li, Yu Li, Tianyu Yang, **Yunfei Liu**, Dongxu Yue, Zhihui Lin, Dong Xu
- `ICLR 2024` GPAvatar: Generalizable and Precise Head Avatar from Image(s), Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, **Yunfei Liu**, Tatsuya Harada
- `PRCV 2024` [Visibility Enhancement for Low-light Hazy Scenarios](https://arxiv.org/abs/2308.00591), Chaoqun Zhuang, **Yunfei Liu**, Sijia Wen, Feng Lu.
- `ICCV 2023` [Accurate 3D Face Reconstruction with Facial Component Tokens](https://liuyunfei.net/), Tianke Zhang, Xuangeng Chu, **Yunfei Liu**, Lijian Lin, Zhendong Yang, _et al._.
- `CVMJ 2023` [Discriminative feature encoding for intrinsic image decomposition](https://phi-ai.buaa.edu.cn/publications/index.htm), Zhongji Wang, **Yunfei Liu**, Feng Lu.
- `CVPR 2022` [Generalizing Gaze Estimation with Rotation Consistency](https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_Generalizing_Gaze_Estimation_With_Rotation_Consistency_CVPR_2022_paper.pdf), Yiwei Bao, **Yunfei Liu**, Haofei Wang, Feng Lu.
- `IEEE-VR 2022` [Reconstructing 3D Virtual Face with Eye Gaze from a Single Image](https://phi-ai.buaa.edu.cn/publications/index.htm), Jiadong Liang, **Yunfei Liu**, Feng Lu. [<font color=Red>Oral</font>]
- `TOMM 2022` [Semantic Guided Single Image Reflection Removal](https://dl.acm.org/doi/10.1145/3510821), **Yunfei Liu**, Yu Li, Shaodi You, Feng Lu, [GitHub](https://github.com/DreamtaleCore/SGRRN).
- `arXiv 2022` [Jitter Does Matter: Adapting Gaze Estimation to New Domains](https://arxiv.org/abs/2210.02082), Mingjie Xu, Haofei Wang, **Yunfei Liu**, Feng Lu.
- `ISAMR 2021` [3D Photography with One-shot Portrait Relighting](https://ieeexplore.ieee.org/document/9585876), **Yunfei Liu**, Sijia Wen, Feng Lu.
- `ISAMR 2021` [Edge-Guided Near-Eye Image Analysis for Head Mounted Displays](https://ieeexplore.ieee.org/document/9583797), Zhimin Wang\*, Yuxin Zhao\*, **Yunfei Liu**, Feng Lu. [<font color=Red>Oral</font>] [GitHub](https://github.com/zhaoyuhsin/Edge-Guided-Near-Eye-Image-Analysis-for-Head-Mounted-Displays), [Demo video](https://youtu.be/wV1kkvdW5WE)
- `BMVC 2021` [Separating Content and Style for Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2110.14404), **Yunfei Liu**, Haofei Wang, Yang Yue, Feng Lu. [GitHub](https://github.com/DreamtaleCore/SCS-UIT).
- `arXiv 2021` [Unsupervised Two-Stage Anomaly Detection](https://arxiv.org/pdf/2103.11671.pdf), **Yunfei Liu**, Chaoqun Zhuang, Feng Lu, [GitHub](https://github.com/DreamtaleCore/UTAD).
- `arXiv 2021` [Cloud Sphere: A 3D Shape Representation via Progressive Deformation](https://arxiv.org/abs/2112.11133), Zongji Wang, **Yunfei Liu**, Feng Lu.
- `arXiv 2021` [Vulnerability of Appearance-based Gaze Estimation](https://arxiv.org/abs/2103.13134), Mingjie Xu, Haofei Wang, **Yunfei Liu**, Feng Lu.
- `AAAI 2020` [Separate In Latent Space: Unsupervised Single Image Layer Separation](https://arxiv.org/abs/1906.00734), **Yunfei Liu**, Feng Lu. [GitHub](https://github.com/DreamtaleCore/SILS) [<font color=Red>Oral</font>]
- `ICPR 2020` [Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets](https://arxiv.org/abs/2103.11119), Yiwei Bao, Yihua Cheng, **Yunfei Liu**, Feng Lu. [GitHub](https://github.com/kirito12138/AFF-Net).
- `ACM-MM 2019` [What I See Is What You See: Joint Attention Learning for First and Third Person Video Co-analysis](https://arxiv.org/abs/1904.07424), Huangyue Yu, Minjie Cai, **Yunfei Liu**, Feng Lu.
